{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from rnn_theano import RNNTheano\n",
    "from gru_theano import GRUTheano\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.load('xtrain.npy')\n",
    "y=np.load('ytrain.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNNTheano(8000, hidden_dim=80)\n",
    "t1 = time.time()\n",
    "model.sgd_step(x[10], y[10], 0.005)\n",
    "t2 = time.time()\n",
    "print(\"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankurgoel/kaggle/datastuff/nn/gru_theano.py:112: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  [x, y, learning_rate, theano.Param(decay, default=0.9)],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Step time: 418.977022 milliseconds\n"
     ]
    }
   ],
   "source": [
    "model = GRUTheano(8000, hidden_dim=80)\n",
    "t1 = time.time()\n",
    "model.sgd_step(x[10], y[10], 0.005)\n",
    "t2 = time.time()\n",
    "print(\"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65752 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'recreational' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading CSV file...\")\n",
    "with open('reddit-comments-2015-08.csv', 'rt',encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.__next__()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (\"Parsed %d sentences.\" % (len(sentences)))\n",
    "     \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print (\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    " \n",
    "print (\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print (\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "np.save('xtrain',X_train)\n",
    "np.save('ytrain',y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
